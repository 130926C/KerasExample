{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compact Convolutional Transformers\n",
    "### 紧凑卷积Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原文链接：https://keras.io/examples/vision/cct/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note：\n",
    "这个案例是用紧凑卷积 Transformers 实现图像分类。总体来说这个案例比较简单，代码量也很少，对 Transformer 了解的话读起来会相当轻松。如果对这个模型不熟悉也没关系，先跟着做下去，因为 Keras Example 的顺序比较乱，后面有一个案例详解了 Transformer 包括具体实现。  \n",
    "\n",
    "目前你需要知道的是，Transformer 最初被用在自然语言处理上，其主要是多头注意力机制构成的编码器和解码器，然后有学者将 Transformer 应用到了图像识别领域中发现效果也非常好。在 Computer Vision 部分先讨论 Vision Transformer（ViT）相关的内容，后面的自然语言处理中会介绍原始用于翻译的 Transormer。  \n",
    "\n",
    "虽然该模型有望成为 CV 和 NLP 中的大一统方案，但该模型在 CV 方面有两个缺陷：  \n",
    "1. 当数据集非常大（百万以上）的时候才会表现出和 ResNet 之间的性能差异。\n",
    "2. 模型时间和空间复杂度均为 $O(n^{2})$。\n",
    "\n",
    "其实第二个缺陷已经非常致命了，所以后面就有大量的魔改版。这个案例的来源就是一篇魔改论文。作者提出了一种结构更为紧凑的 Transformer 模型（**【注意】不是 ViT 模型**），该模型所需的数据集较小，同时也更容易训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the [Vision Transformers (ViT)](https://arxiv.org/abs/2010.11929) paper, a Transformer-based architecture for vision typically requires a larger dataset than usual, as well as a longer pre-training schedule. [ImageNet-1k](http://imagenet.org/)  (which has about a million images) is considered to fall under the medium-sized data regime with respect to ViTs. This is primarily because, unlike CNNs, ViTs (or a typical Transformer-based architecture) do not have well-informed inductive biases (such as convolutions for processing images). This begs the question: can't we combine the benefits of convolution and the benefits of Transformers in a single network architecture? These benefits include parameter-efficiency, and self-attention to process long-range and global dependencies (interactions between different regions in an image).  \n",
    "\n",
    "正如 [Vision Transformers (ViT)](https://arxiv.org/abs/2010.11929) 这篇文章所说的，基于 Transformer 的视觉架构通常需要更大的数据集，以及更长的训练时间。[ImageNet-1k](http://imagenet.org/) （约一百万张）被认为是 ViT 框架下的中型数据集。造成这一现象的原因是 ViT 没有被广泛认同的 **归纳偏置** 能力（如 CNN 的卷积）。这样就引出了一个问题：能否将卷积和 Transformer 的优点结合到一个框架下？这些优点包括了参数训练效率、用于处理大跨度和全局依赖（图像中不同区域之间的联系）的自注意力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的描述中出现了一个关键词 **归纳偏置**。   \n",
    "\n",
    "归纳偏置这个词翻译的有些生硬，我比较认同 知乎博主 **Yulong** 在博客 [深度学习的归纳偏置是什么？](https://www.zhihu.com/question/41404496/answer/1612466556) 所说的 **“归纳偏好”**。当面对一个需要使用神经网络解决的问题时，第一个想到的方案就是 **归纳偏好**。再次引用 **Yulong** 在 [博客](https://www.zhihu.com/question/41404496/answer/1612466556) 中的话：\n",
    "```python\n",
    "1. 深度神经网络（多层感知机）结构就偏好性的认为，层次化处理信息有更好效果；\n",
    "2. 卷积神经网络认为信息具有空间局部性（locality），可以用滑动卷积共享权重方式降低参数空间；\n",
    "3. 反馈神经网络则将时序信息考虑进来强调顺序重要性；\n",
    "4. 图网络则是认为中心节点与邻居节点的相似性会更好引导信息流动。\n",
    "```\n",
    "\n",
    "对于神经网络而言，CNN 的归纳偏好就是卷积操作带来的平移不变性；RNN的归纳偏好是序列的前后时序性。  \n",
    "\n",
    "因此上面这段话的意思是，ViT 没有被广泛地认同用于图像任务中，即 ViT 在小规模数据上表现并不理想，相比较于深度卷积和 ResNet 这类通吃大中小数据集的模型而言，图像处理过程中 ViT 并不是最佳选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [Escaping the Big Data Paradigm with Compact Transformers](https://arxiv.org/abs/2104.05704), Hassani et al. present an approach for doing exactly this. They proposed the Compact Convolutional Transformer (CCT) architecture. In this example, we will work on an implementation of CCT and we will see how well it performs on the CIFAR-10 dataset.  \n",
    "\n",
    "在文章 [Escaping the Big Data Paradigm with Compact Transformers](https://arxiv.org/abs/2104.05704) 中，作者提出了一种紧凑型 Transformer 结构（CCT）来解决其需要大规模数据的问题。这个案例演示了该模型在 CIFAR-10 上的表现。  \n",
    "\n",
    "If you are unfamiliar with the concept of self-attention or Transformers, you can read this chapter from François Chollet's book Deep Learning with Python. This example uses code snippets from another example, [Image classification with Vision Transformer](https://keras.io/examples/vision/image_classification_with_vision_transformer/).  \n",
    "\n",
    "如果你不熟悉 self-attention 或 Transformers，你可以阅读 François Chollet 的书 《Deep Learning with Python》 [这一章节](https://livebook.manning.com/book/deep-learning-with-python-second-edition/chapter-11/r-3/312)，顺便参考 [Image classification with Vision Transformer](https://keras.io/examples/vision/image_classification_with_vision_transformer/) 这个示例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下图展示了他们在论文中提出的模型（CCT）结构。\n",
    "\n",
    "<img src='../images/CV_Img/CCT.png'>  \n",
    "\n",
    "从图中可以发现 ViT 和 CVT 都使用基于 Patch 的标记作为输入，而 CCT 使用的是卷积结果作为标记。如下图所示，将一张图片卷积的结果作为 Patch 以替代通过切割得到的 Patch。  \n",
    "\n",
    "这样做有一个显而易见的好处：卷积操作是使用一个卷积核扫遍整个图得到的一个特征图（Feature Map），那么这个特征图就包含了这张照片的全局信息；而如果是 CVT 和 ViT 切割的话，切割区域不重叠会使得每个 Patch 之间的边缘关系被弱化，同时大跨度的两个 Patch（左上角和右下角） 之间关系则更弱。  \n",
    "\n",
    "<img src='../images/CV_Img/CCT_CVT.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_emb = True\n",
    "conv_layers = 2\n",
    "projection_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 2\n",
    "transformer_units = [\n",
    "    projection_dim,\n",
    "    projection_dim,\n",
    "]\n",
    "transformer_layers = 2\n",
    "stochastic_depth_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "image_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 10)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CCT tokenizer\n",
    "The first recipe introduced by the CCT authors is the tokenizer for processing the images. In a standard ViT, images are organized into uniform non-overlapping patches. This eliminates the boundary-level information present in between different patches. This is important for a neural network to effectively exploit the locality information. The figure below presents an illustration of how images are organized into patches.  \n",
    "\n",
    "CCT 作者介绍的第一个优化是在图像标记器 （Tokenizer）上。在标准 ViT 中，图像被组织成统一的非重叠 Patch。这样做抹除了不同 Patch 之间的边界信息。不同区域之间的边界信息对于神经网络利用局部信息而言很重要。下图展示了传统 Vit 的做法。  \n",
    "\n",
    "<img src='../images/CV_Img/CCT_example.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了解决边界信息丢失的问题，作者使用卷积的方式来生成右边的 Patchs 图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCTTokenizer(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        padding=1,\n",
    "        pooling_kernel_size=3,\n",
    "        pooling_stride=2,\n",
    "        num_conv_layers=conv_layers,\n",
    "        num_output_channels=[64, 128],\n",
    "        positional_emb=positional_emb,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(CCTTokenizer, self).__init__(**kwargs)\n",
    "\n",
    "        # This is our tokenizer. 这是标记器\n",
    "        self.conv_model = keras.Sequential()\n",
    "        for i in range(num_conv_layers):\n",
    "            self.conv_model.add(\n",
    "                layers.Conv2D(\n",
    "                    num_output_channels[i],\n",
    "                    kernel_size,\n",
    "                    stride,\n",
    "                    padding=\"valid\",\n",
    "                    use_bias=False,\n",
    "                    activation=\"relu\",\n",
    "                    kernel_initializer=\"he_normal\",\n",
    "                )\n",
    "            )\n",
    "            self.conv_model.add(layers.ZeroPadding2D(padding))\n",
    "            self.conv_model.add(\n",
    "                layers.MaxPool2D(pooling_kernel_size, pooling_stride, \"same\")\n",
    "            )\n",
    "\n",
    "        self.positional_emb = positional_emb\n",
    "\n",
    "    def call(self, images):\n",
    "        outputs = self.conv_model(images)\n",
    "        # 图像通过上面的卷积层后，将其压平（Flatten）成一个向量\n",
    "        reshaped = tf.reshape(\n",
    "            outputs,\n",
    "            (-1, tf.shape(outputs)[1] * tf.shape(outputs)[2], tf.shape(outputs)[-1]),\n",
    "        )\n",
    "        return reshaped\n",
    "\n",
    "    def positional_embedding(self, image_size):\n",
    "        # 位置编码在CCT中是可选的，如果要做位置编码就返回一个 Embedding 功能层\n",
    "        if self.positional_emb:\n",
    "            dummy_inputs = tf.ones((1, image_size, image_size, 3))\n",
    "            dummy_outputs = self.call(dummy_inputs)\n",
    "            sequence_length = tf.shape(dummy_outputs)[1]\n",
    "            projection_dim = tf.shape(dummy_outputs)[-1]\n",
    "\n",
    "            embed_layer = layers.Embedding(\n",
    "                input_dim=sequence_length, output_dim=projection_dim\n",
    "            )\n",
    "            return embed_layer, sequence_length\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic depth for regularization\n",
    "Stochastic depth is a regularization technique that randomly drops a set of layers. During inference, the layers are kept as they are. It is very much similar to Dropout but only that it operates on a block of layers rather than individual nodes present inside a layer. In CCT, stochastic depth is used just before the residual blocks of a Transformers encoder.  \n",
    "\n",
    "随机深度（Stochastic depth）是一种随机删除一层的正则化技术。在预测过程中并不会删除。这个和 Dropout 非常相似，但 Dropout 是对层内的神经元进行随机休克，而 stochastic depth 是以层为单位进行随机休眠。在 CCT 中，随机深度作用于 Transformer 编码器之前的残差卷积模块中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referred from: github.com:rwightman/pytorch-image-models.\n",
    "class StochasticDepth(layers.Layer):\n",
    "    def __init__(self, drop_prop, **kwargs):\n",
    "        super(StochasticDepth, self).__init__(**kwargs)\n",
    "        self.drop_prob = drop_prop\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if training: # 如果是训练则随机深度，预测则保持原样\n",
    "            keep_prob = 1 - self.drop_prob\n",
    "            shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)\n",
    "            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n",
    "            random_tensor = tf.floor(random_tensor)\n",
    "            return (x / keep_prob) * random_tensor\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP for the Transformers encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "In the original paper, the authors use [AutoAugment](https://arxiv.org/abs/1805.09501) to induce stronger regularization. For this example, we will be using the standard geometric augmentations like random cropping and flipping.  \n",
    "\n",
    "在 [原始论文](https://arxiv.org/abs/2104.05704) 中，作者使用了 [AutoAugment](https://arxiv.org/abs/1805.09501) 进行数据增强。但在这个例子中使用标准的几何增强，如剪裁和翻转。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Rescaling(scale=1.0 / 255),\n",
    "        layers.RandomCrop(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The final CCT model\n",
    "Another recipe introduced in CCT is attention pooling or sequence pooling. In ViT, only the feature map corresponding to the class token is pooled and is then used for the subsequent classification task (or any other downstream task). In CCT, outputs from the Transformers encoder are weighted and then passed on to the final task-specific layer (in this example, we do classification).  \n",
    "\n",
    "CCT 另一个改进是 **注意力池** 或 **序列池**。在 ViT 中，只有与标记对应的特征图被池化，然后用于后续的分类任务。在 CCT 中，Transformer 编码器的加权输出被送入下游任务层中。（在这个例子中是分类）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里有个新名词“注意力池”，其主要功能是将 Transformer Encoder 产生的序列信息进行池化。在下面的代码中是通过 **tf.squeeze()** 函数实现的。该函数能够删除输入的指定维度，此处直接删除了倒数第二个维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cct_model(\n",
    "    image_size=image_size,\n",
    "    input_shape=input_shape,\n",
    "    num_heads=num_heads,\n",
    "    projection_dim=projection_dim,\n",
    "    transformer_units=transformer_units,\n",
    "):\n",
    "\n",
    "    inputs = layers.Input(input_shape)\n",
    "\n",
    "    # Augment data. 数据增强\n",
    "    augmented = data_augmentation(inputs)\n",
    "\n",
    "    # Encode patches. 编码Patch\n",
    "    cct_tokenizer = CCTTokenizer()\n",
    "    encoded_patches = cct_tokenizer(augmented)\n",
    "\n",
    "    # Apply positional embedding. 位置嵌入\n",
    "    if positional_emb:\n",
    "        pos_embed, seq_length = cct_tokenizer.positional_embedding(image_size)\n",
    "        positions = tf.range(start=0, limit=seq_length, delta=1)\n",
    "        position_embeddings = pos_embed(positions)\n",
    "        encoded_patches += position_embeddings\n",
    "\n",
    "    # Calculate Stochastic Depth probabilities. 计算随深度概率\n",
    "    dpr = [x for x in np.linspace(0, stochastic_depth_rate, transformer_layers)]\n",
    "\n",
    "    # Create multiple layers of the Transformer block. 构建多层Transformer\n",
    "    for i in range(transformer_layers):\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        attention_output = StochasticDepth(dpr[i])(attention_output)\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-5)(x2)\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        x3 = StochasticDepth(dpr[i])(x3)\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Apply sequence pooling. 序列池化，删除倒数第二个维度\n",
    "    representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
    "    attention_weights = tf.nn.softmax(layers.Dense(1)(representation), axis=1)\n",
    "    weighted_representation = tf.matmul(\n",
    "        attention_weights, representation, transpose_a=True\n",
    "    )\n",
    "    weighted_representation = tf.squeeze(weighted_representation, -2)\n",
    "\n",
    "    # Classify outputs. 分类输出层\n",
    "    logits = layers.Dense(num_classes)(weighted_representation)\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folderPath = \"tmp/\"\n",
    "if not os.path.exists(folderPath):\n",
    "    os.makedirs(folderPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model):\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=0.0001)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.CategoricalCrossentropy(\n",
    "            from_logits=True, label_smoothing=0.1\n",
    "        ),\n",
    "        metrics=[\n",
    "            keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    checkpoint_filepath = folderPath\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "352/352 [==============================] - 17s 40ms/step - loss: 1.9022 - accuracy: 0.3372 - top-5-accuracy: 0.8285 - val_loss: 1.6442 - val_accuracy: 0.4418 - val_top-5-accuracy: 0.9198\n",
      "Epoch 2/10\n",
      "352/352 [==============================] - 14s 39ms/step - loss: 1.5686 - accuracy: 0.5004 - top-5-accuracy: 0.9315 - val_loss: 1.4918 - val_accuracy: 0.5482 - val_top-5-accuracy: 0.9456\n",
      "Epoch 3/10\n",
      "352/352 [==============================] - 14s 39ms/step - loss: 1.4471 - accuracy: 0.5620 - top-5-accuracy: 0.9486 - val_loss: 1.4273 - val_accuracy: 0.5644 - val_top-5-accuracy: 0.9530\n",
      "Epoch 4/10\n",
      "352/352 [==============================] - 14s 39ms/step - loss: 1.3646 - accuracy: 0.6047 - top-5-accuracy: 0.9579 - val_loss: 1.3147 - val_accuracy: 0.6238 - val_top-5-accuracy: 0.9658\n",
      "Epoch 5/10\n",
      "352/352 [==============================] - 14s 39ms/step - loss: 1.3053 - accuracy: 0.6345 - top-5-accuracy: 0.9645 - val_loss: 1.2620 - val_accuracy: 0.6448 - val_top-5-accuracy: 0.9696\n",
      "Epoch 6/10\n",
      "352/352 [==============================] - 14s 39ms/step - loss: 1.2493 - accuracy: 0.6604 - top-5-accuracy: 0.9708 - val_loss: 1.1924 - val_accuracy: 0.6836 - val_top-5-accuracy: 0.9790\n",
      "Epoch 7/10\n",
      "352/352 [==============================] - 13s 38ms/step - loss: 1.2114 - accuracy: 0.6798 - top-5-accuracy: 0.9726 - val_loss: 1.2067 - val_accuracy: 0.6804 - val_top-5-accuracy: 0.9766\n",
      "Epoch 8/10\n",
      "352/352 [==============================] - 14s 39ms/step - loss: 1.1776 - accuracy: 0.6952 - top-5-accuracy: 0.9760 - val_loss: 1.1733 - val_accuracy: 0.7004 - val_top-5-accuracy: 0.9784\n",
      "Epoch 9/10\n",
      "352/352 [==============================] - 14s 38ms/step - loss: 1.1476 - accuracy: 0.7099 - top-5-accuracy: 0.9783 - val_loss: 1.2331 - val_accuracy: 0.6696 - val_top-5-accuracy: 0.9696\n",
      "Epoch 10/10\n",
      "352/352 [==============================] - 13s 38ms/step - loss: 1.1243 - accuracy: 0.7204 - top-5-accuracy: 0.9795 - val_loss: 1.1168 - val_accuracy: 0.7266 - val_top-5-accuracy: 0.9786\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 1.1303 - accuracy: 0.7236 - top-5-accuracy: 0.9774\n",
      "Test accuracy: 72.36%\n",
      "Test top 5 accuracy: 97.74%\n"
     ]
    }
   ],
   "source": [
    "cct_model = create_cct_model()\n",
    "history = run_experiment(cct_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的 CCT 模型仅有40万个参数，能够快速地完成在 CIFAR-10 上的训练，可以发现仅仅 10 个epoch 就能将准确率提到 % 以上。相比较于 ViT 的470万个参数而言是相当少的。从训练的记录来看，模型的准确率也能迅速提高，训练时间也非常短。"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
