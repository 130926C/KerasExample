{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-supervision and domain adaptation with AdaMatch\n",
    "### 使用AdaMatch统一半监督与（无监督）域适应"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原文链接：https://keras.io/examples/vision/adamatch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note:\n",
    "这个博客实际上做的是通过 **MNIST** 数据集训练模型，然后让模型能够应用到 **SHVN** 数据集上，后者数据集特点是单个样本内容由随机个数、随机颜色的数字图像组成，而 MNIST 数据集单个样本均为一个数字并且是单通道的图像。  \n",
    "\n",
    "总体来说，这个博客上的东西非常值得学习，从博客中可以明显感觉出他有仔细阅读过原文。但整个博主好像是印度裔，导致他的英文有些晦涩，如果遇到了阅读困难或者存在歧义的地方，最好去阅论文[原文](https://arxiv.org/abs/2106.04732)，因为我遇到的几处歧义点也是通过阅读原文解决的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this example, we will implement the AdaMatch algorithm, proposed in [AdaMatch: A Unified Approach to Semi-Supervised Learning and Domain Adaptation](https://arxiv.org/abs/2106.04732) by Berthelot et al. It sets a new state-of-the-art in unsupervised domain adaptation (as of June 2021). AdaMatch is particularly interesting because it unifies semi-supervised learning (SSL) and unsupervised domain adaptation (UDA) under one framework. It thereby provides a way to perform semi-supervised domain adaptation (SSDA).  \n",
    "\n",
    "在这个例子中实现了 AdaMatch 算法，该算法由 **Berthelot** 等人发表的文章 [AdaMatch: A Unified Approach to Semi-Supervised Learning and Domain Adaptation](https://arxiv.org/abs/2106.04732) 中被提出。截至2021年6月前都是最先进的（SOTA）无监督域适应技术。AdaMatch将半监督学习（SSL）和无监督域适应（UDA）统一在一个模型中，因此提供了一种半监督域适应（SSDA）方法。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【注】原始博客中的以下语句\n",
    "```python\n",
    "!pip install tf-models-official\n",
    "```\n",
    "这个语句会帮你自动下载最新的 Tensorflow CPU 版本和对应的 Keras 版本，可能会造成 GPU 版本没有被调用。所以在执行完上述语句后需要再执行以下语句将Keras版本替换回来，并且卸载掉他给你装上的 CPU 版本。\n",
    "```python\n",
    "!pip uninstall tensorflow\n",
    "!pip install keras==2.6.0\n",
    "```\n",
    "如果你发现执行完上述语句中出现以下报错：  \n",
    "**No module named 'tensorflow.python.keras.backend'**  \n",
    "使用以下语句重新安装 GPU 版本的 Tensorflow  \n",
    "```python\n",
    "!pip uninstall tensorflow-gpu\n",
    "!pip install tensorflow-gpu==2.6.0\n",
    "```\n",
    "后面需要用到 official 包中的 RandAugment 工具用于数据增强。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用2.6.0这个版本，是因为这个是目前（2022年2月3日）最新的[通过测试的构建](https://tensorflow.google.cn/install/source#gpu)，我个人也使用这个版本完整测试了该仓库中所有的代码。常用GPU几个版本可以通过上面的链接知晓，也可以通过下标查询。  \n",
    "\n",
    "|版本|Python版本|编译器|构建工具|cuDNN|CUDA|\n",
    "|----|----|---|---|---|---|\n",
    "|2.6.0|3.6~3.9|GCC 7.3.1|Bazel 3.7.2|8.1|11.2|\n",
    "|2.4.0|3.6~3.8|GCC 7.3.1|Bazel 3.1.0|8.0|11.0|\n",
    "|2.2.0|3.5~3.8|GCC 7.3.1|Bazel 2.0.0|7.6|10.1|\n",
    "|2.0.0|2.7、3.3~3.7|GCC 7.3.1|Bazel 0.26.1|7.4|10.0|\n",
    "|1.14.0|2.7、3.3~3.7|GCC 4.8|Bazel 0.24.1|7.4|10.0|\n",
    "\n",
    "尽管Google提供了众多通过测试的版本，但如果能用最新的还是尽可能使用最新的版本，这不仅是因为新版本使用的 CUDA 更现金，还因为新版本增加了很多前沿的工具包，如在 2.6.0 中就有多头注意力层 **MultiHeadAttention** 可以直接拿来使用，省去了重复造轮子的步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "In semi-supervised learning (SSL), we use a small amount of labeled data to train models on a bigger unlabeled dataset. Popular semi-supervised learning methods for computer vision include  [FixMatch](https://arxiv.org/abs/2001.07685), [MixMatch](https://arxiv.org/abs/1905.02249), [Noisy Student Training](https://arxiv.org/abs/1911.04252), etc. You can refer to this example to get an idea of what a standard SSL workflow looks like.  \n",
    "\n",
    "在半监督学习（SSL）中，用少量标记的数据在大量未标记的数据上进行训练。在CV领域中比较流行的半监督方法有 [FixMatch](https://arxiv.org/abs/2001.07685)，[MixMatch](https://arxiv.org/abs/1905.02249)，[Noisy Student Training](https://arxiv.org/abs/1911.04252)等，可以参考 Keras 中 [这个例子](https://keras.io/examples/vision/adamatch/) 来了解基本的半监督学习（SSL）流程是怎样的。  \n",
    "\n",
    "In unsupervised domain adaptation, we have access to a source labeled dataset and a target unlabeled dataset. Then the task is to learn a model that can generalize well to the target dataset. The source and the target datasets vary in terms of distribution. The following figure provides an illustration of this idea. In the present example, we use the MNIST dataset as the source dataset, while the target dataset is SVHN, which consists of images of house numbers. Both datasets have various varying factors in terms of texture, viewpoint, appearence, etc.: their domains, or distributions, are different from one another.  \n",
    "\n",
    "在无监督域适应中，我们可以使用有标签的源数据和无标签的目标数据。任务是训练一个能够很好泛化到目标数据集上的模型。源数据和目标数据在分布上存在不同，下图展示了两者之间的差异。在当前案例中，使用 **MNIST** 作为源数据，**SVHN** 为目标数据，该数据是由门牌号组成，两个数据集图像在纹理、观察点（拍摄角度）、外观（字体和颜色）等方面都存在不同，即两个数据集的域和分布不同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/CV_Img/dJFSJuT.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Popular domain adaptation algorithms in deep learning include [Deep CORAL](https://arxiv.org/abs/1612.01939), [Moment Matching](https://arxiv.org/abs/1812.01754), etc.  \n",
    "在深度学习中的域适应算法有 [Deep CORAL](https://arxiv.org/abs/1612.01939) 和 [Moment Matching](https://arxiv.org/abs/1812.01754) 等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "知乎上的博主 **小蚂蚁曹凯** 在他的文章 [《迁移学习》: 领域自适应(Domain Adaptation)的理论分析](https://zhuanlan.zhihu.com/p/50710267) 上对域适应（**Domain Adaptation**）做出了通俗易懂的解释：  \n",
    "```python\n",
    "Domain Adaptation基本思想是既然源域和目标域数据分布不一样，那么就把数据都映射到一个特征空间中，在特征空间中找一个度量准则，使得源域和目标域数据的特征分布尽量接近，于是基于源域数据特征训练的判别器，就可以用到目标域数据上。\n",
    "```\n",
    "尽管这个博客是从生成对抗网络角度出发进行解释的，但我认为这是最容易让人理解的阐述。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from official.vision.image_classification.augment import RandAugment\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    (mnist_x_train, mnist_y_train),\n",
    "    (mnist_x_test, mnist_y_test),\n",
    ") = keras.datasets.mnist.load_data()\n",
    "\n",
    "# 添加一个通道维度\n",
    "mnist_x_train = tf.expand_dims(mnist_x_train, -1)\n",
    "mnist_x_test = tf.expand_dims(mnist_x_test, -1)\n",
    "\n",
    "# 对标签进行 one-hot 编码\n",
    "mnist_y_train = tf.one_hot(mnist_y_train, 10).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVHN\n",
    "这里使用了 **tensorflow-dataset** 这个库，这也是 Tensorflow 中诸多“开箱即用”的库之一。使用这个库下载好的数据会被存放到 “C:\\Users\\你的用户名\\tensorflow_datasets\\” 路径下，并且以后使用会直接加载本地硬盘。此外，这个工具最好的是它能直接返回 tf.data.Dataset 类型的数据。  \n",
    "\n",
    "这个语句因为涉及到数据集划分，耗时可能会有些长，如果你发现你的 Generating 部分一直卡在 60% 附近请耐心等待，我的电脑初次运行该语句使用了 12 分钟，其中下载数据集使用了约 3 分钟，剩下的时间都是在生成数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\Lucks\\tensorflow_datasets\\svhn_cropped\\3.0.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Size...: 100%|██████████| 1501/1501 [02:07<00:00, 11.81 MiB/s]\n",
      "Dl Completed...: 100%|██████████| 3/3 [02:07<00:00, 42.38s/ url]\n",
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset svhn_cropped downloaded and prepared to C:\\Users\\Lucks\\tensorflow_datasets\\svhn_cropped\\3.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "svhn_train, svhn_test = tfds.load(\n",
    "    \"svhn_cropped\", split=[\"train\", \"test\"], as_supervised=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "svhn_train, svhn_test = tfds.load(\n",
    "    \"svhn_cropped\", split=[\"train\", \"test\"], as_supervised=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constants and hyperparameters\n",
    "### 设置常量及超参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 [原文](extension://bfdogplmndidlpjfhoijckpakkdjkkil/pdf/viewer.html?file=https%3A%2F%2Farxiv.org%2Fpdf%2F2106.04732.pdf) 的第3.2小节中，提到了关于超参数的设置，内容如下：  \n",
    "```python\n",
    "AdaMatch only requires the following two hyper-parameters: \n",
    "AdaMatch仅仅需要以下两个超参数\n",
    "(1) Confidence threshold τ (set to 0.9 for all experiments). \n",
    "    置信阈值 τ (所有实验中都设置为0.9).\n",
    "(2) Unlabeled target batch size ratio uratio (set to 3 for all experiments) which defines how much larger is the unlabeled batch.\n",
    "    目标数据batch和源数据batch的大小比（所有实验中都设置为3）.\n",
    "```\n",
    "这是下段代码中 **TARGET_BATCH_SIZE = 3 * SOURCE_BATCH_SIZE** 的来源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESIZE_TO = 32\n",
    "\n",
    "SOURCE_BATCH_SIZE = 64\n",
    "TARGET_BATCH_SIZE = 3 * SOURCE_BATCH_SIZE  # Reference: Section 3.2\n",
    "EPOCHS = 10\n",
    "STEPS_PER_EPOCH = len(mnist_x_train) // SOURCE_BATCH_SIZE\n",
    "TOTAL_STEPS = EPOCHS * STEPS_PER_EPOCH\n",
    "\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "LEARNING_RATE = 0.03\n",
    "\n",
    "WEIGHT_DECAY = 0.0005\n",
    "INIT = \"he_normal\"\n",
    "DEPTH = 28\n",
    "WIDTH_MULT = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation utilities\n",
    "A standard element of SSL algorithms is to feed weakly and strongly augmented versions of the same images to the learning model to make its predictions consistent. For strong augmentation, RandAugment is a standard choice. For weak augmentation, we will use horizontal flipping and random cropping.  \n",
    "\n",
    "一个常规的 SSL 算法的步骤是将图像的弱增强和强增强数据送入模型，让模型在这两种增强数据上保持一致性。对于强增强而言 [RandAugment](https://arxiv.org/abs/1909.13719) 是常规手段。对于弱增强而言，使用水平翻转和随机剪裁足以。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "博主 **Zerg_Wang** 在博客 [论文笔记：RandAugment](https://blog.csdn.net/Zerg_Wang/article/details/105030503) 中详细讨论了使用 RandAugment 对模型性能提升带来的好处。而 TensorFlow 提供的包中 RandAugment 需要的参数只有2个N和M，相比较于传统数据增强策略动辄几十重而言，RandAugment最大的特点在于“随机”和“强度”。  \n",
    "其中“随机”是通过参数 N 控制的，即从多种增强手段中随机选取一种执行 N 次；  \n",
    "而“强度”是用过参数 M 控制的，即控制每种随机变化的强度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmenter = RandAugment(num_layers=2, magnitude=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tf.tile(input, multiples, name=None)** 维度复制  \n",
    "\n",
    "* input: 输入的向量\n",
    "* multipes: 每个维度需要复制的次数\n",
    "* name: 操作的名，通常默认为 None 就行 \n",
    "\n",
    "其中核心为 multipes 这个参数，在这个例子中的 [1, 1, 3] 意思是将第一个和第二个维度乘以1，第三个维度乘以3。\n",
    "```python\n",
    "example 1: \n",
    "a = np.random.rand(5, 5, 1)  # a.shape=(5, 5, 1)\n",
    "b = tf.tile(a, [1, 1, 3])    # b.shape=(5, 5, 3)\n",
    "\n",
    "example 2:\n",
    "a = np.array([[1,2,3],[4,5,6]])\n",
    "b = tf.tile(a, [2, 3])\n",
    "\n",
    "a = array([[1, 2, 3],\n",
    "           [4, 5, 6]])\n",
    "\n",
    "b = array( [[1, 2, 3, 1, 2, 3, 1, 2, 3],\n",
    "            [4, 5, 6, 4, 5, 6, 4, 5, 6],\n",
    "            [1, 2, 3, 1, 2, 3, 1, 2, 3],\n",
    "            [4, 5, 6, 4, 5, 6, 4, 5, 6]])\n",
    "```\n",
    "需要注意的是，这个函数只是将原有向量的内容进行了复制，并不是真正意义上的灰度转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weak_augment(image, source=True):\n",
    "    if image.dtype != tf.float32:\n",
    "        image = tf.cast(image, tf.float32)\n",
    "\n",
    "    # 如果是 MNIST 输入的话，首先将其从灰度图转为RGB。这里使用的是 tf.tile() 函数对内容复制\n",
    "    if source:\n",
    "        image = tf.image.resize_with_pad(image, RESIZE_TO, RESIZE_TO)  # 用零填充\n",
    "        image = tf.tile(image, [1, 1, 3])\n",
    "    image = tf.image.random_flip_left_right(image)  # 随机左右镜像\n",
    "    image = tf.image.random_crop(image, (RESIZE_TO, RESIZE_TO, 3))  # 随机剪裁\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strong_augment(image, source=True):\n",
    "    if image.dtype != tf.float32:\n",
    "        image = tf.cast(image, tf.float32)\n",
    "\n",
    "    if source:\n",
    "        image = tf.image.resize_with_pad(image, RESIZE_TO, RESIZE_TO)\n",
    "        image = tf.tile(image, [1, 1, 3])\n",
    "    image = augmenter.distort(image)  # 按照之前设置的增强方案完成增强\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**tf.data.Dataset**](https://tensorflow.google.cn/api_docs/python/tf/data/Dataset) 类中最重要的一个函数 [**map()**](https://tensorflow.google.cn/api_docs/python/tf/data/Dataset#map).  \n",
    "\n",
    "这个函数可以将自己写的函数应用到数据集中，用于改造数据值和结构的同时不会打乱原有数据顺序。使用这个函数能让你专注于对单一的数据动手，而不必考虑整体数据的维度和分布，即传入map()中的函数操作单位是单个数据。  \n",
    "\n",
    "```python\n",
    "dataset = Dataset.range(1, 6)           # [ 1, 2, 3, 4, 5 ]\n",
    "dataset = dataset.map(lambda x: x + 1)  # [ 2, 3, 4, 5, 6 ]\n",
    "```\n",
    "从上面的例子可以看出，在 map() 的传入了 lambda表达式，并且表达式操作单元为单个元素 x。  \n",
    "\n",
    "这个函数在后面的例子中会广泛出现，函数中还提供了一个可选参数设置：**num_parallel_calls**，这个参数是指定CPU并行操作数。在实际生产中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_individual_ds(ds, aug_func, source=True):\n",
    "    if source:\n",
    "        batch_size = SOURCE_BATCH_SIZE\n",
    "    else:\n",
    "        batch_size = TARGET_BATCH_SIZE\n",
    "    ds = ds.shuffle(batch_size * 10, seed=42)\n",
    "\n",
    "    if source:\n",
    "        ds = ds.map(lambda x, y: (aug_func(x), y), num_parallel_calls=AUTO)\n",
    "    else:\n",
    "        ds = ds.map(lambda x, y: (aug_func(x, False), y), num_parallel_calls=AUTO)\n",
    "\n",
    "    ds = ds.batch(batch_size).prefetch(AUTO)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_w and _s suffixes denote weak and strong respectively.  \n",
    "在下面这一小段代码中，后缀 \"_w\" 和 \"_s\" 分表代表弱增强和强增强。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对源数据增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_ds = tf.data.Dataset.from_tensor_slices((mnist_x_train, mnist_y_train))\n",
    "source_ds_w = create_individual_ds(source_ds, weak_augment)\n",
    "source_ds_s = create_individual_ds(source_ds, strong_augment)\n",
    "# 用 tf.data.Dataset.zip() 函数可以将两个同类型的Dataset对象组合成一个，\n",
    "# 要求是单个元素的shape和种类要保持一致，允许两个数据集的个数不同。\n",
    "final_source_ds = tf.data.Dataset.zip((source_ds_w, source_ds_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对目标数据增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ds_w = create_individual_ds(svhn_train, weak_augment, source=False)\n",
    "target_ds_s = create_individual_ds(svhn_train, strong_augment, source=False)\n",
    "final_target_ds = tf.data.Dataset.zip((target_ds_w, target_ds_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于在 **create_individual_ds** 函数中使用了 **shuffle()** 这个方法打乱了原有数据集中的顺序，所以此处无法找到对应的弱增强、强增强以及原始数据之间的对应关系，博主在原文中也没有给出源码。但仍然可以通过 **take(N)** 函数对其进行粗略的查看。该函数是从 tf.data.Dataset 对象中取出 **N** 个元素。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_sample(ds):\n",
    "    temp = ds.take(3)\n",
    "    plt.figure(figsize=(6, 2))\n",
    "    i = 1\n",
    "    for img, lab in temp:\n",
    "        plt.subplot(1,3,i)\n",
    "        if len(img.shape) == 3:     # source_ds.shape=(28, 28, 1)\n",
    "            plt.imshow(img)\n",
    "        else:\n",
    "            plt.imshow(tf.cast(img[i], tf.int16))     # source_ds_s/w.shape=(batch_size, 32, 32, 3)\n",
    "        plt.axis(\"off\")\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x144 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "view_sample(source_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x144 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "view_sample(source_ds_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x144 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "view_sample(source_ds_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss computation utilities\n",
    "损失计算函数。  \n",
    "\n",
    "在原文中作者提出了如下图所示的损失函数。  \n",
    "\n",
    "<img src='../images/CV_Img/SemiSuperLoss.png'>  \n",
    "\n",
    "其中 $H(p,q)$ 是 交叉熵损失；stop_gradient 是一个防止梯度在其他参数上反向传播的函数。剩下的具体信息可以参考 [原文](https://arxiv.org/abs/2106.04732)。从上图中的结构可以发现，总体模型的损失值是由 $L_{source}$ 和 $L_{target}$ 线性相加组成的。那么可以定义 $L_{source}$ 和 $L_{target}$ 两个损失计算公式如下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L_{source}$值为源数据集上强弱增强后的分类损失，即模型对源数据增强后的判别损失和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_source(source_labels, logits_source_w, logits_source_s):\n",
    "    loss_func = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    w_loss = loss_func(source_labels, logits_source_w)\n",
    "    s_loss = loss_func(source_labels, logits_source_s)\n",
    "    return w_loss + s_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L_{target}$ 的构成比较复杂。有一个概念是目标数据集上的伪标签（**pseudo labels**），这个是指在半监督或无监督学习中，模型对输入数据集生成的标签，这种生成的标签首先无法保证准确性，同时也无法保证可读性。例如，在无监督学习中模型将输入数字 “9” 赋予一个one-hot向量标签，假设最大概率索引位置为 ‘0’，同样的输入数字 “0” 被赋予一个索引 ‘5’。这是为什么称其为伪标签，这看上去有些不合理，但实际情况是这种做法在 SSL 上十分普遍，KNN算法也采用的是这种逻辑。  \n",
    "\n",
    "首先是将弱增强的模型伪标签和强增强的模型预测标签计算损失，相当于假设模型在弱增强数据上是可靠的，计算强弱增强数据的差异。  \n",
    "\n",
    "然后将上面得到的损失值乘以一个 mask 值，再计算平均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_target(target_pseudo_labels_w, logits_target_s, mask):\n",
    "    loss_func = keras.losses.CategoricalCrossentropy(from_logits=True, reduction=\"none\")\n",
    "    target_pseudo_labels_w = tf.stop_gradient(target_pseudo_labels_w)\n",
    "    target_loss = loss_func(target_pseudo_labels_w, logits_target_s)\n",
    "\n",
    "    mask = tf.cast(mask, target_loss.dtype)\n",
    "    target_loss *= mask\n",
    "    return tf.reduce_mean(target_loss, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subclassed model for AdaMatch training\n",
    "训练AdaMatch子模型。下图展示了AdaMatch模型的训练流程。  \n",
    "\n",
    "<img src='../images/CV_Img/AdaMatchModel.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 以“成对”的方式对源数据和目标数据进行强弱增强。（这里之所以要成对是因为在计算$L_{target}$的时候是用弱增强来矫正强增强，同时此处的成对是只在数据集之内的成对，源数据集和目标数据集之间不存在成对的概念）\n",
    "2. 将源和目标两种强弱增强的数据串联，同时备份一份源数据的串联用于训练子模型。\n",
    "3. 第一次前向传播走的是下面这条支路，即训练混合数据；第二次前向传播走的是上面那条支路，即源数据串联支路。两个支路的输出结构都会经过一次Batch Normalization.\n",
    "4. 两个支路计算各自的logits。（原本logits指的是sigmoid函数，但在神经网络中值得是全连接层输出）。\n",
    "5. 对logits值进行一系列转化（后面说怎么转化）。\n",
    "6. 按照上一小节中的 **Loss function** 计算损失并更新梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建AdaMatch模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个模型中有以下参数需要进行解释：  \n",
    "\n",
    "* 置信阈值：因为上面计算 $L_{target}$ 的时候用了伪标签，引入置信阈值来控制弱增强的可信度。\n",
    "* 预热表：这个是损失函数中的 $\\mu(t)$，用于更新为标记样本贡献的损失权重。\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaMatch(keras.Model):\n",
    "    def __init__(self, model, total_steps, tau=0.9):\n",
    "        super(AdaMatch, self).__init__()\n",
    "        self.model = model\n",
    "        self.tau = tau  # 置信阈值\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        self.total_steps = total_steps\n",
    "        self.current_step = tf.Variable(0, dtype=\"int64\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker]\n",
    "\n",
    "    # 预热表\n",
    "    def compute_mu(self):\n",
    "        pi = tf.constant(np.pi, dtype=\"float32\")\n",
    "        step = tf.cast(self.current_step, dtype=\"float32\")\n",
    "        return 0.5 - tf.cos(tf.math.minimum(pi, (2 * pi * step) / self.total_steps)) / 2\n",
    "\n",
    "    def train_step(self, data):\n",
    "        ## 拆包 & 组织数据 ##\n",
    "        source_ds, target_ds = data\n",
    "        (source_w, source_labels), (source_s, _) = source_ds\n",
    "        (\n",
    "            (target_w, _),\n",
    "            (target_s, _),\n",
    "        ) = target_ds  # 在这里并没有标签\n",
    "\n",
    "        combined_images = tf.concat([source_w, source_s, target_w, target_s], 0)\n",
    "        combined_source = tf.concat([source_w, source_s], 0)\n",
    "\n",
    "        total_source = tf.shape(combined_source)[0]\n",
    "        total_target = tf.shape(tf.concat([target_w, target_s], 0))[0]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            ## 前向传播 ##\n",
    "            combined_logits = self.model(combined_images, training=True)\n",
    "            z_d_prime_source = self.model(\n",
    "                combined_source, training=False\n",
    "            )  # 无BN的更新\n",
    "            z_prime_source = combined_logits[:total_source]\n",
    "\n",
    "            ## 1. 对源数据进行随机均匀插值 ##\n",
    "            lambd = tf.random.uniform((total_source, 10), 0, 1)\n",
    "            final_source_logits = (lambd * z_prime_source) + (\n",
    "                (1 - lambd) * z_d_prime_source\n",
    "            )\n",
    "\n",
    "            ## 2. 分布对齐（仅考虑弱增强数据） ##\n",
    "            # 计算弱增强源数据softmax后的输出.\n",
    "            y_hat_source_w = tf.nn.softmax(final_source_logits[: tf.shape(source_w)[0]])\n",
    "\n",
    "            # 计算弱增强目标数据softmax的输出.\n",
    "            logits_target = combined_logits[total_source:]\n",
    "            logits_target_w = logits_target[: tf.shape(target_w)[0]]\n",
    "            y_hat_target_w = tf.nn.softmax(logits_target_w)\n",
    "\n",
    "            # 将目标标签分布与源数据标签分布对齐\n",
    "            expectation_ratio = tf.reduce_mean(y_hat_source_w) / tf.reduce_mean(\n",
    "                y_hat_target_w\n",
    "            )\n",
    "            y_tilde_target_w = tf.math.l2_normalize(\n",
    "                y_hat_target_w * expectation_ratio, 1\n",
    "            )\n",
    "\n",
    "            ## 3. 相对置信阈值 ##\n",
    "            row_wise_max = tf.reduce_max(y_hat_source_w, axis=-1)\n",
    "            final_sum = tf.reduce_mean(row_wise_max, 0)\n",
    "            c_tau = self.tau * final_sum\n",
    "            mask = tf.reduce_max(y_tilde_target_w, axis=-1) >= c_tau\n",
    "\n",
    "            ## 计算损失 ##\n",
    "            source_loss = compute_loss_source(\n",
    "                source_labels,\n",
    "                final_source_logits[: tf.shape(source_w)[0]],\n",
    "                final_source_logits[tf.shape(source_w)[0] :],\n",
    "            )\n",
    "            target_loss = compute_loss_target(\n",
    "                y_tilde_target_w, logits_target[tf.shape(target_w)[0] :], mask\n",
    "            )\n",
    "\n",
    "            t = self.compute_mu()  # 计算目标损失的权重\n",
    "            total_loss = source_loss + (t * target_loss)\n",
    "            self.current_step.assign_add(\n",
    "                1\n",
    "            )\n",
    "\n",
    "        gradients = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "\n",
    "        self.loss_tracker.update_state(total_loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作者在文中提出了以下三大改进：  \n",
    "\n",
    "* 模型执行两次前向传播，其中只有一个负责更新 Batch Normalization 后的信息。这样做是考虑到目标数据集中的分布变化；在另一个前向传播中仅使用源数据，并且BN层仅在判断的情况下运行。源数据最终的输出是通过两次前向传播输出的线性插值计算的。这可以让分布保持一致。（个人理解为通过这种插值方法可以让模型对无标签样本的输出映射到正确的标签上）\n",
    "* 分布对齐用于对齐源标签分布和目标标签分布，帮助模型学习目标数据的域不变表示。在无监督域适应的问题上，我们无法知道模型内部对目标数据标签是如何处理的。\n",
    "* 为了防止模型生成错误的伪标签影响到整个模型，使用阈值来过滤高置信度的预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate a Wide-ResNet-28-2\n",
    "作者使用了Wide-ResNet-28-2作为AdaMatch结构图中Model的部分。在下面代码中有一个归一化层，将图像的值域控制在 [0, 1] 之间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wide_basic(x, n_input_plane, n_output_plane, stride):\n",
    "    conv_params = [[3, 3, stride, \"same\"], [3, 3, (1, 1), \"same\"]]\n",
    "\n",
    "    n_bottleneck_plane = n_output_plane\n",
    "\n",
    "    # 残差模块\n",
    "    for i, v in enumerate(conv_params):\n",
    "        if i == 0:\n",
    "            if n_input_plane != n_output_plane:\n",
    "                x = layers.BatchNormalization()(x)\n",
    "                x = layers.Activation(\"relu\")(x)\n",
    "                convs = x\n",
    "            else:\n",
    "                convs = layers.BatchNormalization()(x)\n",
    "                convs = layers.Activation(\"relu\")(convs)\n",
    "            convs = layers.Conv2D(\n",
    "                n_bottleneck_plane,\n",
    "                (v[0], v[1]),\n",
    "                strides=v[2],\n",
    "                padding=v[3],\n",
    "                kernel_initializer=INIT,\n",
    "                kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n",
    "                use_bias=False,\n",
    "            )(convs)\n",
    "        else:\n",
    "            convs = layers.BatchNormalization()(convs)\n",
    "            convs = layers.Activation(\"relu\")(convs)\n",
    "            convs = layers.Conv2D(\n",
    "                n_bottleneck_plane,\n",
    "                (v[0], v[1]),\n",
    "                strides=v[2],\n",
    "                padding=v[3],\n",
    "                kernel_initializer=INIT,\n",
    "                kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n",
    "                use_bias=False,\n",
    "            )(convs)\n",
    "\n",
    "    # 根据输入的shape使用不同的短接方式\n",
    "    if n_input_plane != n_output_plane:\n",
    "        shortcut = layers.Conv2D(\n",
    "            n_output_plane,\n",
    "            (1, 1),\n",
    "            strides=stride,\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=INIT,\n",
    "            kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n",
    "            use_bias=False,\n",
    "        )(x)\n",
    "    else:\n",
    "        shortcut = x\n",
    "\n",
    "    return layers.Add()([convs, shortcut])\n",
    "\n",
    "\n",
    "# 堆叠Residual单元\n",
    "def block_series(x, n_input_plane, n_output_plane, count, stride):\n",
    "    x = wide_basic(x, n_input_plane, n_output_plane, stride)\n",
    "    for i in range(2, int(count + 1)):\n",
    "        x = wide_basic(x, n_output_plane, n_output_plane, stride=1)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_network(image_size=32, num_classes=10):\n",
    "    n = (DEPTH - 4) / 6\n",
    "    n_stages = [16, 16 * WIDTH_MULT, 32 * WIDTH_MULT, 64 * WIDTH_MULT]\n",
    "\n",
    "    inputs = keras.Input(shape=(image_size, image_size, 3))\n",
    "    x = layers.Rescaling(scale=1.0 / 255)(inputs)\n",
    "\n",
    "    conv1 = layers.Conv2D(\n",
    "        n_stages[0],\n",
    "        (3, 3),\n",
    "        strides=1,\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=INIT,\n",
    "        kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n",
    "        use_bias=False,\n",
    "    )(x)\n",
    "\n",
    "    ## 添加宽残差块 ##\n",
    "\n",
    "    conv2 = block_series(\n",
    "        conv1,\n",
    "        n_input_plane=n_stages[0],\n",
    "        n_output_plane=n_stages[1],\n",
    "        count=n,\n",
    "        stride=(1, 1),\n",
    "    )  # Stage 1\n",
    "\n",
    "    conv3 = block_series(\n",
    "        conv2,\n",
    "        n_input_plane=n_stages[1],\n",
    "        n_output_plane=n_stages[2],\n",
    "        count=n,\n",
    "        stride=(2, 2),\n",
    "    )  # Stage 2\n",
    "\n",
    "    conv4 = block_series(\n",
    "        conv3,\n",
    "        n_input_plane=n_stages[2],\n",
    "        n_output_plane=n_stages[3],\n",
    "        count=n,\n",
    "        stride=(2, 2),\n",
    "    )  # Stage 3\n",
    "\n",
    "    batch_norm = layers.BatchNormalization()(conv4)\n",
    "    relu = layers.Activation(\"relu\")(batch_norm)\n",
    "\n",
    "    # 决策\n",
    "    trunk_outputs = layers.GlobalAveragePooling2D()(relu)\n",
    "    outputs = layers.Dense(\n",
    "        num_classes, kernel_regularizer=regularizers.l2(WEIGHT_DECAY)\n",
    "    )(trunk_outputs)\n",
    "\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 1.471226 Million parameters.\n"
     ]
    }
   ],
   "source": [
    "wrn_model = get_network()\n",
    "print(f\"Model has {wrn_model.count_params()/1e6} Million parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate AdaMatch model and compile it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = keras.optimizers.schedules.CosineDecay(LEARNING_RATE, TOTAL_STEPS, 0.25)\n",
    "optimizer = keras.optimizers.Adam(reduce_lr)\n",
    "\n",
    "adamatch_trainer = AdaMatch(model=wrn_model, total_steps=TOTAL_STEPS)\n",
    "adamatch_trainer.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "382/382 [==============================] - 111s 259ms/step - loss: 219129085952.0000\n",
      "Epoch 2/10\n",
      "382/382 [==============================] - 99s 258ms/step - loss: 4.0577\n",
      "Epoch 3/10\n",
      "382/382 [==============================] - 99s 258ms/step - loss: 7.0211\n",
      "Epoch 4/10\n",
      "382/382 [==============================] - 99s 259ms/step - loss: 21.2773\n",
      "Epoch 5/10\n",
      "382/382 [==============================] - 99s 259ms/step - loss: 31.0304\n",
      "Epoch 6/10\n",
      "382/382 [==============================] - 99s 259ms/step - loss: 23.2544\n",
      "Epoch 7/10\n",
      "382/382 [==============================] - 99s 259ms/step - loss: 30.5407\n",
      "Epoch 8/10\n",
      "382/382 [==============================] - 99s 258ms/step - loss: 24.7555\n",
      "Epoch 9/10\n",
      "382/382 [==============================] - 99s 259ms/step - loss: 25.5500\n",
      "Epoch 10/10\n",
      "382/382 [==============================] - 99s 258ms/step - loss: 25.9064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x211a9fedb80>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_ds = tf.data.Dataset.zip((final_source_ds, final_target_ds))\n",
    "adamatch_trainer.fit(total_ds, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on the target and source test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 4s 27ms/step - loss: 591.4323 - sparse_categorical_accuracy: 0.0712\n",
      "Accuracy on target test set: 7.12%\n"
     ]
    }
   ],
   "source": [
    "adamatch_trained_model = adamatch_trainer.model\n",
    "adamatch_trained_model.compile(metrics=keras.metrics.SparseCategoricalAccuracy())\n",
    "\n",
    "svhn_test = svhn_test.batch(TARGET_BATCH_SIZE).prefetch(AUTO)\n",
    "_, accuracy = adamatch_trained_model.evaluate(svhn_test)\n",
    "print(f\"Accuracy on target test set: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 2s 27ms/step - loss: 591.4321 - sparse_categorical_accuracy: 0.7174\n",
      "Accuracy on source test set: 71.74%\n"
     ]
    }
   ],
   "source": [
    "def prepare_test_ds_source(image, label):\n",
    "    image = tf.image.resize_with_pad(image, RESIZE_TO, RESIZE_TO)\n",
    "    image = tf.tile(image, [1, 1, 3])\n",
    "    return image, label\n",
    "\n",
    "\n",
    "source_test_ds = tf.data.Dataset.from_tensor_slices((mnist_x_test, mnist_y_test))\n",
    "source_test_ds = (\n",
    "    source_test_ds.map(prepare_test_ds_source, num_parallel_calls=AUTO)\n",
    "    .batch(TARGET_BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "_, accuracy = adamatch_trained_model.evaluate(source_test_ds)\n",
    "print(f\"Accuracy on source test set: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "博主在他的 [GitHub](https://github.com/sayakpaul/AdaMatch-TF/releases/tag/v1.0.0) 中提供了训练好的模型权重。"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
