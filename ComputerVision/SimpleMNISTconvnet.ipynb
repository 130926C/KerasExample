{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple MNIST convnet\n",
    "### 一个简单的MNIST卷积网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原文链接：https://keras.io/examples/vision/mnist_convnet/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note:\n",
    "这个案例其实更应该被当作第一个示例，因为他足够简单，但在这个样例中我会引入一些其他元素来改变程序结构，特别是在数据处理阶段，读者可以对照着原文进行阅读。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span id='pd'>Prepare the data</span>\n",
    "从这部分开始会和原文存在一定出入，在[第一个样例](ImageClassificationFromScratch.ipynb)中使用了 **image_dataset_from_directory** 来构建“流”式数据集，这里同样使用来实现流式的思想。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (28, 28, 1)\n",
    "batch_size = 256\n",
    "num_classes = 10\n",
    "epochs = 5\n",
    "\n",
    "(x_train, y_train), (x_val, y_val) = keras.datasets.mnist.load_data()\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_val = np.expand_dims(x_val, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow 提供了 [**from_tensor_slices**](https://tensorflow.google.cn/api_docs/python/tf/data/Dataset#from_tensor_slices)方法，该方法能够根据输入的tensor进行切片，并且返回的是一个 **tf.data.Dataset** 类型的数据。  \n",
    "\n",
    "用这种方式最大的好处在于可以以“数据集”的角度进行处理，而不必考虑data和label的之间的映射关系，后期在图像数据增强上能够极大简化代码量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "    .shuffle(1024)\n",
    ")\n",
    "\n",
    "val_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "    .shuffle(1024)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的代码中可以看出，使用这种方法可以通过链式方法完成数据预处理，以“打乱数据”这个操作为例，在百度中最常见的方法是如下操作：\n",
    "```python\n",
    "    import random\n",
    "\n",
    "    index = [i for i in range(len(data))]   # 生成顺序的索引\n",
    "    random.shuffle(index)                   # 将顺序索引打乱\n",
    "    \n",
    "    data = data[index]                      # 用乱序索引重新排序 data 和 label\n",
    "    label = label[index]\n",
    "```\n",
    "\n",
    "而使用了**tf.data.Dataset**这个对象后，打乱顺序只需要一句简单的命令 **.shuffle(1024)**。在后面的例子中会出现 **.map()** 方法，这个功能更加强大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "在构建模型时，将标准化操作嵌入模型 Rescaling() 层，如果你使用的是GPU那么这要比原文中的操作更好。  \n",
    "\n",
    "原文中将标准化操作全部给CPU完成了，如下所示：  \n",
    "```python  \n",
    "    x_train = x_train.astype(\"float32\") / 255\n",
    "    x_test = x_test.astype(\"float32\") / 255\n",
    "```\n",
    "这意味着你需要确定以下两件事：  \n",
    "1. 内存有足够的空间来存储这些预处理过后的数据\n",
    "2. 你不介意IO阻塞的问题\n",
    "\n",
    "这样的处理在小型数据集如CIFAR-10/100和MNIST上是完全可以的。但在实际应用中，将所有数据一次性读入内存并不现实，这个过程相当漫长并且存在一定风险。  \n",
    "\n",
    "假设你的数据是10000张PNG格式的图像，一张50KB大小的图片被读入内存中可能会膨胀到2MB大小，这是由于读入时存储类型和图像压缩算法导致的。在内存和硬盘之间有一个swap，当虚拟地址空间不足时会和硬盘进行交换。此时原本保存在硬盘上不足490MB大小的图片集，一次性全部读入内存就可能会变成19.5GB，这种膨胀率是相当恐怖的，你会发现你的硬盘可用空间以肉眼可见的速度减少，当sawp将硬盘可用空间完全占满后仍然不够时，有一定概率导致系统崩溃。所以为了你系统的安全最好从一开始就养成“流”式的想法，每当阅读到这种一次性读入的代码时可以动手将其改造下，多练习几次就可以信手拈来。  \n",
    "\n",
    "这个思想在本例中体现不明显，因为 **mnist.load_data()** 已经将所有数据全部读入内存了，但最好在初期就养成习惯，这将对后面的应用打下坚实的基础。\n",
    "\n",
    "回顾第一个案例 [Image classification from scratch](ImageClassificationFromScratch.ipynb) 中使用的 **image_dataset_from_directory** 函数其实也不是一次性将所有数据都读入，函数返回的对象只是记录了图片的存储路径，所以可以发现执行对应的语句几乎是在一瞬间完成的，而如果将一万张图片读入内存是不可能在几秒内完成。  \n",
    "\n",
    "所以正确的思路是尽量避免因为预处理而引入额外的内存占用，将这些操作打包进模型中让GPU实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "rescaling (Rescaling)        (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                16010     \n",
      "=================================================================\n",
      "Total params: 34,826\n",
      "Trainable params: 34,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Rescaling(1.0/255.0),        # 将标准化流入到模型中\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Tensorflow中有两个容易混淆的[损失函数](https://keras.io/api/losses/)和[评价指标](https://keras.io/api/metrics/)。  \n",
    "\n",
    "|loss |metrics|one-hot|\n",
    "|---|---|---|\n",
    "|SparseCategoricalCrossentropy()|SparseCategoricalAccuracy()|No|\n",
    "|CategoricalCrossentropy()|CategoricalAccuracy()|Yes|\n",
    "\n",
    "原文中在[Prepare the data](#pd)部分多了如下操作：\n",
    "```python  \n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "```\n",
    "这个操作就是one-hot编码，将0-9的数字数字标签映射到了一个 shape=(10, ) 向量上。  \n",
    "经历过one-hot编码后，标签“1”和“4”分别被编码成如下形式：  \n",
    "```python  \n",
    "    label \"1\": [0,1,0,0,0,0,0,0,0,0]\n",
    "    label \"4\": [0,0,0,0,1,0,0,0,0,0]\n",
    "```\n",
    "\n",
    "在此处不对标签进行one-hot编码，为的是让读者理解两类损失函数和评价指标之间的差异以及应用场景。  \n",
    "\n",
    "这里还需要注意的一点是：无论是使用 **SparseCategoricalCrossentropy** 还是 **CategoricalCrossentropy**，在最后一全连接层中神经元的个数应该是的类别总数，而不能认为因为使用了 **SparseCategoricalCrossentropy** 后这个数值变成1。具体可以参考 [链接](https://tensorflow.google.cn/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n",
    "    optimizer=tf.keras.optimizers.Adam(), \n",
    "    metrics=tf.keras.metrics.SparseCategoricalAccuracy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "235/235 [==============================] - 4s 7ms/step - loss: 0.4921 - sparse_categorical_accuracy: 0.8543\n",
      "Epoch 2/5\n",
      "235/235 [==============================] - 2s 6ms/step - loss: 0.1340 - sparse_categorical_accuracy: 0.9598\n",
      "Epoch 3/5\n",
      "235/235 [==============================] - 2s 6ms/step - loss: 0.0952 - sparse_categorical_accuracy: 0.9704\n",
      "Epoch 4/5\n",
      "235/235 [==============================] - 2s 6ms/step - loss: 0.0800 - sparse_categorical_accuracy: 0.9753\n",
      "Epoch 5/5\n",
      "235/235 [==============================] - 2s 6ms/step - loss: 0.0704 - sparse_categorical_accuracy: 0.9780\n"
     ]
    }
   ],
   "source": [
    "histroy = model.fit(train_ds, epochs=epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 4ms/step - loss: 0.0423 - sparse_categorical_accuracy: 0.9853\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(val_ds, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
